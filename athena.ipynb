{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fd5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9ec192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Configuration\n",
    "AWS_REGION = 'us-east-1'\n",
    "DATABASE = 'a05'\n",
    "TABLE = 'a05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1752e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Athena client\n",
    "athena_client = boto3.client('athena', region_name=AWS_REGION)\n",
    "\n",
    "# Function to execute Athena query\n",
    "def execute_athena_query(query, output_location=None):\n",
    "    \"\"\"\n",
    "    Execute an Athena query and return results as DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        query: SQL query string\n",
    "        output_location: S3 location for query results\n",
    "        \n",
    "    Returns:\n",
    "        pandas DataFrame with query results\n",
    "    \"\"\"\n",
    "    # Generate unique output location if not provided\n",
    "    if output_location is None:\n",
    "        timestamp = str(int(time.time()))\n",
    "        output_location = f\"s3://athena-cg1372/query-results/{timestamp}/\"\n",
    "    \n",
    "    # Start query execution\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': DATABASE},\n",
    "        ResultConfiguration={'OutputLocation': output_location}\n",
    "    )\n",
    "    \n",
    "    query_id = response['QueryExecutionId']\n",
    "    print(f\"Started query execution: {query_id}\")\n",
    "    \n",
    "    # Wait for query completion\n",
    "    while True:\n",
    "        response = athena_client.get_query_execution(QueryExecutionId=query_id)\n",
    "        status = response['QueryExecution']['Status']['State']\n",
    "        \n",
    "        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            break\n",
    "            \n",
    "        print(f\"Query status: {status}\")\n",
    "        time.sleep(2)\n",
    "    \n",
    "    if status != 'SUCCEEDED':\n",
    "        raise Exception(f\"Query failed: {response['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "    \n",
    "    # Get query results\n",
    "    results_response = athena_client.get_query_results(QueryExecutionId=query_id)\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    columns = [col['VarCharValue'] for col in results_response['ResultSet']['Rows'][0]['Data']]\n",
    "    rows = []\n",
    "    \n",
    "    for row in results_response['ResultSet']['Rows'][1:]:\n",
    "        row_data = [col.get('VarCharValue', '') for col in row['Data']]\n",
    "        rows.append(row_data)\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d707e61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Problem 1: Top 10 subreddits by comment count ===\n",
      "Started query execution: de1683af-a608-46ac-abf9-a91429019283\n",
      "Query status: QUEUED\n",
      "Problem 1 completed. Results saved to prob1_results.csv\n",
      "       subreddit comment_count\n",
      "0      AskReddit        177871\n",
      "1       facepalm         83841\n",
      "2  AmItheAsshole         81354\n",
      "3            nba         60028\n",
      "4        TrueFMK         59138\n"
     ]
    }
   ],
   "source": [
    "# Problem 1: Top 10 subreddits by comment count\n",
    "print(\"=== Problem 1: Top 10 subreddits by comment count ===\")\n",
    "\n",
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    subreddit, \n",
    "    COUNT(*) as comment_count\n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "WHERE subreddit IS NOT NULL \n",
    "    AND subreddit != ''\n",
    "GROUP BY subreddit\n",
    "ORDER BY comment_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df1 = execute_athena_query(query1)\n",
    "df1.to_csv('prob1_results.csv', index=False)\n",
    "print(\"Problem 1 completed. Results saved to prob1_results.csv\")\n",
    "print(df1.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494bbfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Problem 2: 10 random rows ===\n",
      "Started query execution: e80af15d-8b2c-4960-8ce1-9054dd3e9189\n",
      "Query status: QUEUED\n",
      "Query status: RUNNING\n",
      "Query status: RUNNING\n",
      "Query status: RUNNING\n",
      "Query status: RUNNING\n",
      "Problem 2 completed. Results saved to prob2_results.csv\n",
      "                 author author_flair_css_class      author_flair_text  \\\n",
      "0              shinneui                         :Ravenclaw: Ravenclaw   \n",
      "1       chiraqmusicwiki                                                 \n",
      "2  Affectionate-Air7554                                                 \n",
      "3       slightlybearish                                                 \n",
      "4              Honestnt                                                 \n",
      "\n",
      "                                                body controversiality  \\\n",
      "0  Was he actually drunk? I seem to remember that...                0   \n",
      "1  they don’t false claim like that fr, most garl...                0   \n",
      "2  First of all, let’s stop and take a breath. Fe...                0   \n",
      "3  Anyone disinterested in this final is on some ...                0   \n",
      "4  What do you think of the fan theory that Tom B...                0   \n",
      "\n",
      "  created_utc distinguished edited gilded       id     link_id   parent_id  \\\n",
      "0  1685651888                           0  jmixnre  t3_13xbyfr  t1_jmgixqw   \n",
      "1  1685581567                           0  jmf1vrr  t3_13wz4rp  t1_jmf1gw6   \n",
      "2  1685649178                           0  jmiqgrb  t3_13vv2qt  t3_13vv2qt   \n",
      "3  1685658894                           0  jmjf57w  t3_13x6dv4  t1_jmi5da0   \n",
      "4  1685647068                           0  jmikuhm  t3_13xq298  t1_jmiht5p   \n",
      "\n",
      "  retrieved_on score stickied          subreddit subreddit_id  \n",
      "0   1692669748     2    false  hogwartslegacyJKR    t5_7ugbjt  \n",
      "1   1692639873     6    false     LoneStarGhetto    t5_31gtof  \n",
      "2   1692668320     1    false           politics     t5_2cneq  \n",
      "3   1692673178     2    false           NHLMemes     t5_2um9i  \n",
      "4   1692667211     1    false          AskReddit     t5_2qh1i  \n"
     ]
    }
   ],
   "source": [
    "# Problem 2: 10 random rows from comments table\n",
    "print(\"\\n=== Problem 2: 10 random rows ===\")\n",
    "\n",
    "query2 = \"\"\"\n",
    "SELECT *\n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "ORDER BY RAND()\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df2 = execute_athena_query(query2)\n",
    "df2.to_csv('prob2_results.csv', index=False)\n",
    "print(\"Problem 2 completed. Results saved to prob2_results.csv\")\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c95fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Problem 3: Comments per day per hour ===\n",
      "Started query execution: 6c21911b-3e4d-4e6d-a11b-446c528b01d8\n",
      "Query status: QUEUED\n",
      "Problem 3 completed. Results saved to prob3_results.csv\n",
      "  comment_date comment_hour comment_count\n",
      "0   2023-06-01           16        442534\n",
      "1   2023-06-01           17        439124\n",
      "2   2023-06-01           18        438994\n",
      "3   2023-06-01           19        433847\n",
      "4   2023-06-01           20        431052\n"
     ]
    }
   ],
   "source": [
    "# Problem 3: Comments per day per hour\n",
    "print(\"\\n=== Problem 3: Comments per day per hour ===\")\n",
    "\n",
    "query3 = \"\"\"\n",
    "SELECT \n",
    "    DATE_FORMAT(FROM_UNIXTIME(created_utc), '%Y-%m-%d') as comment_date,\n",
    "    HOUR(FROM_UNIXTIME(created_utc)) as comment_hour,\n",
    "    COUNT(*) as comment_count\n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "WHERE created_utc IS NOT NULL\n",
    "GROUP BY \n",
    "    DATE_FORMAT(FROM_UNIXTIME(created_utc), '%Y-%m-%d'),\n",
    "    HOUR(FROM_UNIXTIME(created_utc))\n",
    "ORDER BY comment_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df3 = execute_athena_query(query3)\n",
    "df3.to_csv('prob3_results.csv', index=False)\n",
    "print(\"Problem 3 completed. Results saved to prob3_results.csv\")\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f0ccc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Problem 4: Top 10 subreddits by average score ===\n",
      "Started query execution: e4edc035-bee6-43fe-a06f-3de28f762985\n",
      "Query status: QUEUED\n",
      "Problem 4 completed. Results saved to prob4_results.csv\n",
      "               subreddit           avg_score\n",
      "0                Fauxmoi   58.81477298396205\n",
      "1  BestofRedditorUpdates  55.067522838607175\n",
      "2                 movies   52.95817774458551\n",
      "3             rpdrcringe  52.891304347826086\n",
      "4     nevertellmetheodds  52.321100917431195\n"
     ]
    }
   ],
   "source": [
    "# Problem 4: Top 10 subreddits by average score\n",
    "print(\"\\n=== Problem 4: Top 10 subreddits by average score ===\")\n",
    "\n",
    "query4 = \"\"\"\n",
    "SELECT \n",
    "    subreddit,\n",
    "    AVG(score) as avg_score\n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "WHERE subreddit IS NOT NULL \n",
    "    AND subreddit != ''\n",
    "    AND score IS NOT NULL\n",
    "GROUP BY subreddit\n",
    "HAVING COUNT(*) >= 100  -- Only include subreddits with at least 100 comments\n",
    "ORDER BY avg_score DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df4 = execute_athena_query(query4)\n",
    "df4.to_csv('prob4_results.csv', index=False)\n",
    "print(\"Problem 4 completed. Results saved to prob4_results.csv\")\n",
    "print(df4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963fab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Problem 5: Most controversial comments in r/datascience ===\n",
      "Started query execution: 64e273cd-f3a3-4d41-bb38-80902b0288e2\n",
      "Query status: QUEUED\n",
      "Query status: RUNNING\n",
      "Problem 5 completed. Results saved to prob5_results.csv\n",
      "        author                                               body score  \\\n",
      "0       MrEloi  Why did you continue after, say, the first 30 ...    -4   \n",
      "1   miciomacho  You guys, you know, Nate Silver told me that j...    -2   \n",
      "2       Tam27_  Do you have any templates that you use to mess...    -1   \n",
      "3    bassabyss  I’m surprised no one has mentioned Lex Fridman...    -1   \n",
      "4  Odd-One8023  It's against the rules of the subreddit. On to...     1   \n",
      "\n",
      "  controversiality  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                1  \n",
      "4                1  \n"
     ]
    }
   ],
   "source": [
    "# Problem 5: Most controversial comments in r/datascience\n",
    "print(\"\\n=== Problem 5: Most controversial comments in r/datascience ===\")\n",
    "\n",
    "query5 = \"\"\"\n",
    "SELECT \n",
    "    author,\n",
    "    body,\n",
    "    score,\n",
    "    controversiality\n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "WHERE subreddit = 'datascience'\n",
    "    AND controversiality = 1\n",
    "    AND body IS NOT NULL\n",
    "    AND body NOT IN ('[deleted]', '[removed]')\n",
    "ORDER BY score ASC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "df5 = execute_athena_query(query5)\n",
    "df5.to_csv('prob5_results.csv', index=False)\n",
    "print(\"Problem 5 completed. Results saved to prob5_results.csv\")\n",
    "print(df5.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e86040bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query for extracting high-quality AI/GenAI comments\n",
    "query6a = \"\"\"\n",
    "SELECT\n",
    "    subreddit,\n",
    "    author,\n",
    "    body,\n",
    "    score,\n",
    "    created_utc,\n",
    "    controversiality\n",
    "FROM \"AwsDataCatalog\".\"a05\".\"a05\"\n",
    "WHERE (\n",
    "    REGEXP_LIKE(LOWER(subreddit), '^(artificial|artificialintelligence|ai|machinelearning|ml|deeplearning)$') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '^(chatgpt|openai|gpt|claude|anthropic|bard)$') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '^(midjourney|stablediffusion|dalle|dalle2|aiart|comfyui)$') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '^(tensorflow|pytorch|keras|huggingface|localllama)$') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '^(singularity|agi|automation|aiethics)$') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '.*artificial.*intelligence.*') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '.*machine.*learning.*') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '.*deep.*learning.*') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '.*generative.*ai.*') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '.*chat.*gpt.*') OR\n",
    "    REGEXP_LIKE(LOWER(subreddit), '.*stable.*diffusion.*')\n",
    ")\n",
    "AND LENGTH(body) BETWEEN 100 AND 1000\n",
    "AND score >= 2\n",
    "AND body NOT IN ('[deleted]', '[removed]')\n",
    "AND body IS NOT NULL\n",
    "AND author IS NOT NULL\n",
    "AND author != 'AutoModerator'\n",
    "ORDER BY score DESC, created_utc DESC\n",
    "\"\"\"\n",
    "\n",
    "# Save the SQL query to file for Part A\n",
    "with open('problem6_query.sql', 'w') as f:\n",
    "    f.write(query6a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment-athena",
   "language": "python",
   "name": "assignment-athena"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
